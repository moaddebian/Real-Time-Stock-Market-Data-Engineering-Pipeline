# ğŸ“ˆ Real-Time Stock Market Data Engineering Pipeline

<div align="center">

![Python](https://img.shields.io/badge/Python-3.8+-blue?style=for-the-badge&logo=python&logoColor=white)
![Kafka](https://img.shields.io/badge/Apache-Kafka-231F20?style=for-the-badge&logo=apache-kafka&logoColor=white)
![FastAPI](https://img.shields.io/badge/FastAPI-009688?style=for-the-badge&logo=fastapi&logoColor=white)
![Streamlit](https://img.shields.io/badge/Streamlit-FF4B4B?style=for-the-badge&logo=streamlit&logoColor=white)
![Docker](https://img.shields.io/badge/Docker-2496ED?style=for-the-badge&logo=docker&logoColor=white)
![AWS](https://img.shields.io/badge/AWS-232F3E?style=for-the-badge&logo=amazon-aws&logoColor=white)
![License](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)

**Un pipeline complet d'ingÃ©nierie de donnÃ©es en temps rÃ©el pour l'analyse de donnÃ©es boursiÃ¨res**

[Features](#-fonctionnalitÃ©s-principales) â€¢ [Installation](#-installation-rapide) â€¢ [Documentation](#-documentation) â€¢ [Contribuer](#-contribution)

</div>

---

## ğŸ“– Ã€ Propos

Ce projet est une **solution complÃ¨te d'ingÃ©nierie de donnÃ©es en temps rÃ©el** qui simule et traite des donnÃ©es boursiÃ¨res en utilisant Apache Kafka comme systÃ¨me de messagerie distribuÃ©. Il intÃ¨gre des fonctionnalitÃ©s avancÃ©es d'analytics, de visualisation interactive, et une API REST complÃ¨te pour l'analyse et l'interrogation des donnÃ©es.

### ğŸ¯ Objectifs du Projet

- âœ… IngÃ©rer des donnÃ©es boursiÃ¨res en temps rÃ©el via Kafka
- âœ… Calculer automatiquement des indicateurs techniques (SMA, EMA, RSI, VolatilitÃ©)
- âœ… DÃ©tecter des anomalies et alertes en temps rÃ©el
- âœ… Stocker les donnÃ©es de maniÃ¨re optimisÃ©e (format Parquet)
- âœ… Visualiser les donnÃ©es via un dashboard interactif
- âœ… Exposer les donnÃ©es via une API REST documentÃ©e
- âœ… IntÃ©grer avec AWS (S3, Glue, Athena) pour l'analyse Ã  grande Ã©chelle

## ğŸ“‘ Table des MatiÃ¨res

- [Ã€ Propos](#-Ã -propos)
- [FonctionnalitÃ©s Principales](#-fonctionnalitÃ©s-principales)
- [Architecture](#-architecture)
- [PrÃ©requis](#-prÃ©requis)
- [Installation Rapide](#-installation-rapide)
- [Utilisation](#-utilisation)
- [FonctionnalitÃ©s DÃ©taillÃ©es](#-fonctionnalitÃ©s-dÃ©taillÃ©es)
- [Endpoints API](#-endpoints-api)
- [Configuration](#-configuration)
- [Tests et Validation](#-tests-et-validation)
- [Structure du Projet](#-structure-du-projet)
- [DÃ©ploiement AWS](#-dÃ©ploiement-aws-optionnel)
- [Performances](#-performances-et-optimisations)
- [DÃ©pannage](#-dÃ©pannage)
- [Apprentissages ClÃ©s](#-apprentissages-clÃ©s)
- [Upcoming features](#-upcoming-features)
- [Contribution](#-contribution)
- [License](#-license)
- [Auteur](#-auteur)

---

## ğŸŒŸ FonctionnalitÃ©s Principales

### ğŸš€ Pipeline de DonnÃ©es
- **Streaming Kafka** : Ingestion et traitement de donnÃ©es en temps rÃ©el
- **Analytics Automatiques** : Calcul d'indicateurs techniques (SMA, EMA, RSI, VolatilitÃ©)
- **Alertes Intelligentes** : DÃ©tection automatique de variations de prix et pics de volume
- **Batch Processing** : Traitement optimisÃ© par lots pour meilleures performances
- **Format Parquet** : Compression et stockage optimisÃ©s (90% d'Ã©conomie d'espace)

### ğŸ“Š Visualisation & API
- **Dashboard Interactif** : Interface Streamlit avec graphiques en temps rÃ©el
- **API REST** : FastAPI avec documentation Swagger automatique
- **Graphiques AvancÃ©s** : Chandeliers japonais, indicateurs techniques, volumes

### ğŸ—ï¸ Infrastructure
- **Docker Compose** : DÃ©ploiement Kafka en une commande
- **Configuration CentralisÃ©e** : Fichier YAML et variables d'environnement
- **AWS Integration** : Support S3, Glue, et Athena
- **Production-Ready** : Gestion d'erreurs, retry logic, logging structurÃ©

## ğŸ›ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Dataset    â”‚  Stock Market Historical Data (CSV)
â”‚     CSV      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Enhanced Producerâ”‚  â€¢ Technical Indicators Calculation
â”‚    + Analytics   â”‚  â€¢ Alert Detection (Price/Volume)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â€¢ Structured JSON Logging
         â”‚
         â”‚ JSON Messages
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Apache Kafka    â”‚  â€¢ Distributed Message Queue
â”‚    + Zookeeper   â”‚  â€¢ Topic: stock_market_data
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â€¢ High Throughput & Reliability
         â”‚
         â”‚ Real-time Streaming
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Enhanced Consumerâ”‚  â€¢ Batch Processing (100 msg/batch)
â”‚   + Parquet      â”‚  â€¢ Parquet Format (90% compression)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â€¢ S3 Partitioning (date/symbol)
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Storage Layer (S3 / Local)           â”‚
â”‚  output/stock_market_batch_*.parquet         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚          â”‚          â”‚           â”‚
    â–¼          â–¼          â–¼           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Streamlitâ”‚ â”‚FastAPIâ”‚ â”‚AWS Glue â”‚ â”‚ Athena  â”‚
â”‚Dashboardâ”‚ â”‚  API  â”‚ â”‚ Crawler â”‚ â”‚   SQL   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“‹ PrÃ©requis

- **Python** 3.8 ou supÃ©rieur
- **Docker** et Docker Compose
- **8GB RAM** minimum recommandÃ©
- **AWS CLI** configurÃ© 

## ğŸš€ Installation Rapide

### 1. Cloner le repository

```bash
git clone https://github.com/moaddebian/stock-market-kafka-data-engineering-project.git
cd stock-market-kafka-data-engineering-project
```

> **Note** : Assurez-vous d'avoir Git installÃ© sur votre systÃ¨me.

### 2. Installer les dÃ©pendances

```bash
# CrÃ©er un environnement virtuel (recommandÃ©)
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate     # Windows

# Installer les packages
pip install -r requirements.txt
```

### 3. DÃ©marrer Kafka avec Docker

```bash
docker-compose up -d
```

Cela dÃ©marre :
- âœ… Kafka Broker (port 9092)
- âœ… Zookeeper (port 2181)
- âœ… Kafka UI (port 8080) - Interface web de gestion

### 4. VÃ©rifier l'installation

```bash
# VÃ©rifier les containers
docker ps

# AccÃ©der Ã  Kafka UI
# Ouvrir http://localhost:8080 dans votre navigateur
```

## ğŸ’» Utilisation

### ğŸš€ Quick Start

Pour dÃ©marrer rapidement le pipeline complet :

```bash
# 1. DÃ©marrer Kafka
docker-compose up -d

# 2. Dans un terminal sÃ©parÃ© - DÃ©marrer le Producer
python -m src.producer_enhanced

# 3. Dans un autre terminal - DÃ©marrer le Consumer
python -m src.consumer_enhanced

# 4. Dans un autre terminal - DÃ©marrer le Dashboard
streamlit run dashboard.py

# 5. Dans un autre terminal - DÃ©marrer l'API (optionnel)
uvicorn api.main:app --host 127.0.0.1 --port 8000 --reload
```

### DÃ©marrer le Pipeline Complet

**Terminal 1 - Producer** :
```bash
python -m src.producer_enhanced
```
- âœ… Envoie des messages avec analytics Ã  Kafka
- âœ… Calcule les indicateurs techniques en temps rÃ©el
- âœ… DÃ©tecte et log les alertes automatiquement

**Terminal 2 - Consumer** :
```bash
python -m src.consumer_enhanced
```
- âœ… Lit les messages de Kafka
- âœ… Sauvegarde en batch format Parquet
- âœ… Supporte S3 et stockage local

**Terminal 3 - Dashboard** :
```bash
streamlit run dashboard.py
```
- ğŸŒ Ouvrir http://localhost:8501
- âœ… Visualisation en temps rÃ©el
- âœ… Graphiques interactifs
- âœ… MÃ©triques et indicateurs

**Terminal 4 - API REST** :
```bash
uvicorn api.main:app --host 127.0.0.1 --port 8000 --reload
```
- ğŸŒ API : http://localhost:8000
- ğŸ“š Documentation Swagger : http://localhost:8000/docs
- ğŸ“– Documentation ReDoc : http://localhost:8000/redoc
- âœ… Health Check : http://localhost:8000/health

## ğŸ“Š FonctionnalitÃ©s DÃ©taillÃ©es

### Analytics en Temps RÃ©el

Le systÃ¨me calcule automatiquement :

| Indicateur | Description | PÃ©riode |
|-----------|-------------|---------|
| **SMA** | Simple Moving Average | 20 et 50 jours |
| **EMA** | Exponential Moving Average | 12 jours |
| **RSI** | Relative Strength Index | 14 jours |
| **VolatilitÃ©** | Ã‰cart-type des rendements | 20 jours |
| **Changement de Prix** | Variation en pourcentage | Jour Ã  jour |
| **Volume Moyen** | Moyenne mobile du volume | 20 jours |

### SystÃ¨me d'Alertes Automatiques

DÃ©tection en temps rÃ©el de :

- ğŸš¨ **Variations de Prix** : Alertes si changement > 5% ou > 10%
- ğŸ“Š **Pics de Volume** : Alertes si volume > 200% de la moyenne
- ğŸ“‰ **RSI ExtrÃªmes** : Surachat (>70) ou Survente (<30)

### Format Parquet OptimisÃ©

**Avantages** :
- ğŸ’¾ **Compression** : Jusqu'Ã  90% de rÃ©duction de taille vs JSON
- âš¡ **Performance** : RequÃªtes 10-100x plus rapides
- ğŸ’° **CoÃ»ts** : RÃ©duction significative des coÃ»ts AWS S3/Athena
- ğŸ¯ **Typage Fort** : SchÃ©ma strict avec types de donnÃ©es

**Exemple de gain** :
```
JSON  : 1000 messages = 500 KB
Parquet : 1000 messages = 50 KB
Ã‰conomie : 90%
```

## ğŸ”Œ Endpoints API

### Informations GÃ©nÃ©rales
- `GET /` - Informations API et liste des endpoints
- `GET /health` - Health check et statut du systÃ¨me

### DonnÃ©es BoursiÃ¨res
- `GET /stats` - Statistiques globales (messages, symboles, prix moyens)
- `GET /data` - DonnÃ©es avec filtres (pagination, symbole, dates)
- `GET /symbols` - Liste de tous les symboles disponibles
- `GET /symbol/{symbol}` - DonnÃ©es pour un symbole spÃ©cifique
- `GET /indicators/{symbol}` - Indicateurs techniques d'un symbole

### Exemples de RequÃªtes

```bash
# Obtenir les statistiques globales
curl http://localhost:8000/stats

# DonnÃ©es d'un symbole avec pagination
curl "http://localhost:8000/data?symbol=HSI&limit=50"

# Indicateurs techniques d'un symbole
curl http://localhost:8000/indicators/HSI

# DonnÃ©es avec filtre de date
curl "http://localhost:8000/data?start_date=2020-01-01&limit=100"
```

## âš™ï¸ Configuration

### Fichier config.yaml

Tous les paramÃ¨tres sont configurables via `config/config.yaml` :

```yaml
kafka:
  bootstrap_servers: "localhost:9092"
  topic_name: "stock_market_data"

producer:
  send_interval: 1  # secondes entre messages
  max_messages: null  # null = infini

consumer:
  output_format: "parquet"  # json ou parquet
  batch_size: 100
  use_s3: false  # true pour AWS S3
  local_output_dir: "output"

analytics:
  enabled: true
  calculate_indicators: true
  alert_thresholds:
    price_change_percent: 5.0
    volume_spike_percent: 200.0
```

### Variables d'Environnement

CrÃ©er un fichier `.env` :

```bash
# Kafka
KAFKA_BOOTSTRAP_SERVERS=localhost:9092
KAFKA_TOPIC_NAME=stock_market_data

# Consumer
CONSUMER_OUTPUT_FORMAT=parquet
CONSUMER_BATCH_SIZE=100

# AWS (optionnel)
AWS_S3_BUCKET=your-bucket-name
AWS_ACCESS_KEY_ID=your-access-key
AWS_SECRET_ACCESS_KEY=your-secret-key
```

## ğŸ§ª Tests et Validation

### Tester Kafka

```bash
# Lister les topics
docker exec -it kafka kafka-topics --list --bootstrap-server localhost:9092

# VÃ©rifier les messages
docker exec -it kafka kafka-console-consumer \
  --topic stock_market_data \
  --bootstrap-server localhost:9092 \
  --from-beginning
```

### VÃ©rifier les Outputs

```bash
# Lister les fichiers Parquet gÃ©nÃ©rÃ©s
ls -lh output/  # Linux/Mac
dir output\     # Windows

# Lire un fichier Parquet avec Python
python -c "import pandas as pd; df = pd.read_parquet('output/stock_market_batch_1_*.parquet'); print(df.head())"
```

### Tester l'API

```bash
# Health check
curl http://localhost:8000/health

# Statistiques
curl http://localhost:8000/stats

# DonnÃ©es d'un symbole
curl "http://localhost:8000/symbol/HSI?limit=10"
```

## ğŸ“ Structure du Projet

```
stock-market-kafka-pipeline/
â”‚
â”œâ”€â”€ ğŸ“„ README.md                   # Documentation principale
â”œâ”€â”€ ğŸ“„ LICENSE                     # Licence MIT
â”œâ”€â”€ ğŸ“„ requirements.txt            # DÃ©pendances Python
â”œâ”€â”€ ğŸ“„ docker-compose.yml          # Infrastructure Kafka
â”œâ”€â”€ ğŸ“„ dashboard.py                # Dashboard Streamlit
â”œâ”€â”€ ğŸ“„ .gitignore                  # Fichiers ignorÃ©s par Git
â”œâ”€â”€ ğŸ“„ indexProcessed.csv          # Dataset boursier
â”‚
â”œâ”€â”€ ğŸ“ src/                        # Code source principal
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py                  # Configuration centralisÃ©e
â”‚   â”œâ”€â”€ analytics.py               # Calcul d'indicateurs techniques
â”‚   â”œâ”€â”€ producer_enhanced.py       # Producer amÃ©liorÃ©
â”‚   â””â”€â”€ consumer_enhanced.py       # Consumer amÃ©liorÃ©
â”‚
â”œâ”€â”€ ğŸ“ api/                        # API REST FastAPI
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ main.py                    # Application FastAPI
â”‚
â”œâ”€â”€ ğŸ“ config/                     # Configuration
â”‚   â””â”€â”€ config.yaml                # Configuration YAML principale
â”‚
â”œâ”€â”€ ğŸ“ scripts/                    # Scripts utilitaires
â”‚   â””â”€â”€ setup.sh                   # Script d'installation
â”‚
â”œâ”€â”€ ğŸ“ docs/                       # Documentation
â”‚   â””â”€â”€ command_kafka.txt          # Commandes Kafka de rÃ©fÃ©rence
â”‚
â”œâ”€â”€ ğŸ“ output/                     # DonnÃ©es gÃ©nÃ©rÃ©es (ignorÃ© par Git)
â”‚   â””â”€â”€ stock_market_batch_*.parquet
â”‚
â””â”€â”€ ğŸ“ venv/                       # Environnement virtuel (ignorÃ© par Git)
```

## ğŸŒ DÃ©ploiement AWS 

### PrÃ©requis AWS

1. CrÃ©er un bucket S3
2. Configurer AWS CLI : `aws configure`
3. CrÃ©er un Glue Crawler 

### Configuration pour S3

Dans `config/config.yaml` :

```yaml
consumer:
  use_s3: true
  s3_bucket: "your-bucket-name"
  output_format: "parquet"
  partition_by: ["date", "index"]
```

### AWS Glue et Athena

1. **Glue Crawler** : DÃ©couvre automatiquement le schÃ©ma des donnÃ©es S3
2. **Glue Data Catalog** : Catalogue de mÃ©tadonnÃ©es centralisÃ©
3. **Athena** : ExÃ©cutez des requÃªtes SQL sur vos donnÃ©es S3

```sql
-- Exemple de requÃªte Athena
SELECT 
    Index,
    Date,
    Close,
    indicators_sma_20,
    indicators_rsi_14
FROM stock_market_table
WHERE Date >= '2023-01-01'
ORDER BY Date DESC
LIMIT 100;
```

## ğŸ“ˆ Performances et Optimisations

### CapacitÃ©s du Pipeline

- **Producer** : ~1000 messages/seconde
- **Consumer** : ~5000 messages/seconde (batch processing)
- **Kafka** : Millions de messages/seconde (scalable)

### Optimisations ImplÃ©mentÃ©es

1. **Batch Processing** : Ã‰criture par lots (100 messages)
2. **Format Parquet** : Compression columnar (90% Ã©conomie)
3. **Partitionnement S3** : Organisation par date/symbole
4. **Cache API** : Cache des donnÃ©es (30s TTL)
5. **Async Producer** : Envoi non-bloquant

### CoÃ»ts AWS EstimÃ©s

| Service | CoÃ»t Mensuel (estimation) |
|---------|--------------------------|
| S3 Storage (100 GB) | $2.30 |
| Athena Queries (1 TB scanned) | $5.00 |
| Glue Crawler (1h/jour) | $13.20 |
| **Total** | **~$20.50** |

## ğŸ“¸ Screenshots & DÃ©monstration

### Dashboard Streamlit
- ğŸ“Š Visualisation en temps rÃ©el des donnÃ©es boursiÃ¨res
- ğŸ“ˆ Graphiques interactifs (chandeliers, volumes, indicateurs techniques)
- ğŸ” Filtrage par symbole et pÃ©riode
- ğŸ“‰ MÃ©triques principales et statistiques

### API REST FastAPI
- ğŸ“š Documentation Swagger interactive (OpenAPI)
- ğŸ”Œ Endpoints RESTful pour interroger les donnÃ©es
- âœ… Health checks et monitoring intÃ©grÃ©s
- ğŸš€ Performance optimisÃ©e avec cache

> **Note** : Des captures d'Ã©cran seront ajoutÃ©es prochainement pour illustrer le dashboard et l'API.

## ğŸ› DÃ©pannage

### Kafka ne dÃ©marre pas

```bash
# VÃ©rifier les containers
docker ps -a

# Voir les logs
docker-compose logs kafka

# RedÃ©marrer
docker-compose down
docker-compose up -d
```

### Pas de donnÃ©es dans le Dashboard

1. âœ… VÃ©rifier que le consumer a crÃ©Ã© des fichiers dans `output/`
2. âœ… VÃ©rifier le chemin dans la sidebar du dashboard
3. âœ… VÃ©rifier les logs du consumer

### Erreur d'import

```bash
# RÃ©installer les dÃ©pendances
pip install --upgrade -r requirements.txt
```

### Port dÃ©jÃ  utilisÃ©

```bash
# Changer le port dans config/config.yaml
# Ou utiliser un autre port :
streamlit run dashboard.py --server.port 8502
uvicorn api.main:app --host 127.0.0.1 --port 8001
```

## ğŸ“ Apprentissages ClÃ©s

Ce projet dÃ©montre et enseigne :

### Technologies & Concepts

- âœ… **Real-Time Data Processing** : Kafka, streaming, event-driven architecture
- âœ… **Data Engineering** : ETL, batch processing, data pipelines
- âœ… **Financial Analytics** : Indicateurs techniques, sÃ©ries temporelles, volatilitÃ©
- âœ… **Cloud Computing** : AWS S3, Glue, Athena, intÃ©gration cloud-native
- âœ… **API Development** : REST API avec FastAPI, documentation automatique (Swagger/OpenAPI)
- âœ… **Data Visualization** : Dashboard interactif avec Streamlit, graphiques en temps rÃ©el
- âœ… **DevOps** : Docker, Docker Compose, containerisation, orchestration
- âœ… **Software Engineering** : Architecture modulaire, configuration externalisÃ©e, logging structurÃ©
- âœ… **Data Formats** : Parquet vs JSON, optimisation de stockage, compression
- âœ… **Monitoring & Observability** : Health checks, mÃ©triques, logging

## ğŸš€ Upcoming features 

### Roadmap Future

Ce projet peut Ãªtre Ã©tendu avec :

1. **ğŸ§ª Tests AutomatisÃ©s** : Unitaires et d'intÃ©gration avec pytest
2. **ğŸ”„ CI/CD Pipeline** : GitHub Actions pour tests et dÃ©ploiement automatique
3. **ğŸ“Š Monitoring AvancÃ©** : Prometheus + Grafana pour mÃ©triques en temps rÃ©el
4. **ğŸ¤– ML Predictions** : ModÃ¨le de prÃ©diction de prix (LSTM, Prophet, Transformer)
5. **ğŸ“ˆ Backtesting** : Simulation de stratÃ©gies de trading avec mÃ©triques de performance
6. **ğŸŒ Multi-Sources** : IntÃ©gration avec APIs rÃ©elles (Alpha Vantage, Yahoo Finance, Polygon.io)
7. **ğŸ’¾ Time-Series DB** : InfluxDB ou TimescaleDB pour optimisation des requÃªtes temporelles
8. **ğŸ”’ Security** : Authentification JWT, chiffrement TLS, ACLs Kafka
9. **ğŸ“± Notifications** : Alertes par email, Slack, ou webhooks
10. **ğŸ”„ Real-time ML** : Scoring de modÃ¨les ML en streaming avec Kafka Streams


## ğŸ‘¤ Auteur

<div align="center">

**MOAD DABYANE**

Data Engineer & Software Developer

[![GitHub](https://img.shields.io/badge/GitHub-181717?style=for-the-badge&logo=github&logoColor=white)](https://github.com/moaddebian)
[![LinkedIn](https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/dabyane-moad/)
[![Email](https://img.shields.io/badge/Email-D14836?style=for-the-badge&logo=gmail&logoColor=white)](mailto:mouaddebien@gmail.com)

</div>

## ğŸ“š Documentation

Pour plus de dÃ©tails sur :
- **Architecture dÃ©taillÃ©e** : Voir la section [Architecture](#-architecture)
- **Configuration avancÃ©e** : Voir la section [Configuration](#-configuration)
- **DÃ©ploiement AWS** : Voir la section [DÃ©ploiement AWS](#-dÃ©ploiement-aws-optionnel)
- **Commandes Kafka** : Voir `docs/command_kafka.txt`

<div align="center">

Made with â¤ï¸ and â˜• by MOAD DABYANE

[â¬† Retour en haut](#-real-time-stock-market-data-engineering-pipeline)

</div>
